# GPT

默认存储路径：/home/ssfc/.cache/huggingface



## distilbert/distilgpt2

https://huggingface.co/distilbert/distilgpt2

DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters. 

DistilGPT2本地存储空间340M。



Comment:  这样看来distilgpt2并不比gpt2小多少嘛。(2024年2月9日)



## openai-community/gpt2

https://huggingface.co/openai-community/gpt2

This is the **smallest** version of GPT-2, with 124M parameters.

GPT2本地存储空间526M。

124/82 = 1.512

526/340 = 1.547

参数量的比值和本地存储空间的比值还挺接近。(2024年2月9日)



## openai-community/gpt2-medium

https://huggingface.co/openai-community/gpt2-medium

GPT-2 Medium is the **355M parameter** version of GPT-2. 

gpt2-medium本地存储空间1.5G。

355/124 = 2.863

1.5/0.526 = 2.852

这个参数量的比值和本地存储空间的比值也挺接近。(2024年2月9日)



## openai-community/gpt2-large

https://huggingface.co/openai-community/gpt2-large

GPT-2 Large is the **774M parameter** version of GPT-2. 

gpt2-medium本地存储空间3.1G。(2024年2月9日)



## openai-community/gpt2-xl

https://huggingface.co/openai-community/gpt2-xl

GPT-2 XL is the **1.5B parameter** version of GPT-2. 

gpt2-xl本地存储空间6G。(2024年2月9日)

Comment:  模型越大，推理时间也会越长。(2024年2月9日)



## bigcode/starcoder

https://huggingface.co/bigcode/starcoder

https://www.bilibili.com/video/BV1mm4y1x72Q/?spm_id_from=333.337.search-card.all.click&vd_source=3ef4175721f926fbf390a069da19b0ca

代码预训练模型。

Comment:  原来评测大模型可以用huggingface evaluate库。(2024年2月9日)




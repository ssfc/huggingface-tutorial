# GPT

默认存储路径：/home/ssfc/.cache/huggingface



## distilbert/distilgpt2

https://huggingface.co/distilbert/distilgpt2

DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters. 

DistilGPT2本地存储空间340M。



Comment:  这样看来distilgpt2并不比gpt2小多少嘛。(2024年2月9日)

## openai-community/gpt2

https://huggingface.co/openai-community/gpt2

This is the **smallest** version of GPT-2, with 124M parameters.



## openai-community/gpt2-medium

https://huggingface.co/openai-community/gpt2-medium

GPT-2 Medium is the **355M parameter** version of GPT-2. 



## openai-community/gpt2-large

https://huggingface.co/openai-community/gpt2-large

GPT-2 Large is the **774M parameter** version of GPT-2. 



## openai-community/gpt2-xl

https://huggingface.co/openai-community/gpt2-xl

GPT-2 XL is the **1.5B parameter** version of GPT-2. 





## bigcode/starcoder

https://huggingface.co/bigcode/starcoder

https://www.bilibili.com/video/BV1mm4y1x72Q/?spm_id_from=333.337.search-card.all.click&vd_source=3ef4175721f926fbf390a069da19b0ca

代码预训练模型。

Comment:  原来评测大模型可以用huggingface evaluate库。(2024年2月9日)




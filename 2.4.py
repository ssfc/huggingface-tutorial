


# Section 1: Word-based
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)

# Section 2: Character-based

# Section 3: Subword tokenization











